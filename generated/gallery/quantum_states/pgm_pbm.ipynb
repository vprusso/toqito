{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Pretty Good and Pretty Bad Measurements\n===========================================\n\nIn this tutorial, we will explore the \"pretty good measurement\" (PGM) and its\nnovel counterpart, the \"pretty bad measurement\" (PBM). The PGM, also known as the\nsquare-root measurement, is a widely used measurement for quantum\nstate discrimination [@belavkin1975optimal][@hughston1993complete]. The PBM, in contrast,\nwas recently introduced by McIrvin et. al. [@mcirvin2024pretty]. Both these measurements\nprovide elegant, easy-to-construct tools for two opposing goals in quantum\ninformation: state discrimination and state exclusion.\nPGM is useful for the former while PBM is of use for the latter.\n\nWe will verify their core properties and replicate some of the key numerical\nresults and figures from the paper using `|toqito\u27e9`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background: Discrimination vs. Exclusion\n\nBob wins the standard **quantum state discrimination** task, if he successfully guesses the state sent by Alice.\nAlice is sending Bob a quantum state $\\rho_i$ chosen from an ensemble\n$\\{(p_i, \\rho_i)\\}_{i=1}^k$ known to Bob. Bob's goal is to perform a measurement\nthat maximizes his probability of correctly guessing the index $i$.\nThe best possible probability, $P_{\\text{Best}}$, is the maximum success\nprobability achievable over all possible measurements (POVMs) $\\{M_i\\}$.\n\n$$\nP_{\\text{Best}} = \\max \\sum_{i=1}^k p_i \\text{Tr}(M_i \\rho_i)\n$$\n\nHowever, finding $P_{\\text{Best}}$ is often computationally very hard. The \"pretty good measurement\" (PGM) is a well-established heuristic for this task.\nIts measurement operators $G_i$ are constructed from the ensemble as:\n\n$$\nG_i = P^{-1/2} (p_i \\rho_i) P^{-1/2} \\quad \\text{where} \\quad P = \\sum_{i=1}^k p_i \\rho_i\n$$\n\nThe success probability when using the PGM is given by the standard Born rule, averaged over the ensemble:\n\n$$\nP_{\\text{PGM}} = \\sum_{i=1}^k p_i \\text{Tr}(\\rho_i G_i)\n$$\n\nThe **state exclusion** task is the opposite: Bob wins if he correctly guesses\na state that Alice *did not* send. This is equivalent to minimizing the\nprobability of correctly guessing the state Alice *did* send. This minimum\nachievable success probability is denoted $P_{\\text{Worst}}$:\n\n$$\nP_{\\text{Worst}} = \\min \\sum_{i=1}^k p_i \\text{Tr}(M_i \\rho_i)\n$$\n\nThe \"pretty bad measurement\" (PBM) is a heuristic designed to approximate\nthis worst-case performance. The PBM is elegantly defined in terms of the\nPGM operators $G_i$. In the formula below, $k$ is the number of states in the ensemble,\nand $\\mathbb{I}$ is the identity operator with the same dimensions as the states:\n\n$$\nB_i = \\frac{1}{k-1}(\\mathbb{I} - G_i)\n$$\n\nThe success probability for discrimination when using the PBM is, analogously:\n\n$$\nP_{\\text{PBM}} = \\sum_{i=1}^k p_i \\text{Tr}(\\rho_i B_i)\n$$\n\nA key result from McIrvin et.al [@mcirvin2024pretty] is the tight relationship\nbetween the success probabilities of these two measurements:\n\n$$\nP_{\\text{PGM}} + (k-1)P_{\\text{PBM}} = 1\n$$\n\nThis implies a performance hierarchy against the optimal probabilities and the\nblind guessing probability ($1/k$):\n\n$$\nP_{\\text{Best}} \\ge P_{\\text{PGM}} \\ge \\frac{1}{k} \\ge P_{\\text{PBM}} \\ge P_{\\text{Worst}}\n$$\n\nWe will verify this hierarchy with a concrete example.\n\n### Numerical Example: The Trine States\n\nFigure 3 from McIrvin et.al [@mcirvin2024pretty] analyzes the performance\nof these measurements for the three **trine states** with a uniform prior\nprobability. The trine states are a classic example of a set that is\nantidistinguishable but not distinguishable, a property demonstrated in the [Quantum state exclusion](../state_exclusion) tutorial.\n\nOur plan is to:\n\n1.  Generate the trine states and assume uniform prior probabilities.\n2.  Compute the optimal win/loss probabilities, $P_{\\text{Best}}$\n    and $P_{\\text{Worst}}$, using `|toqito\u27e9`'s SDP solvers.\n3.  Construct the PGM and PBM measurement operators.\n4.  Calculate the success probabilities $P_{\\text{PGM}}$ and $P_{\\text{PBM}}$.\n5.  Print and compare all values, verifying the performance hierarchy and\n    the relationship between $P_{\\text{PGM}}$ and $P_{\\text{PBM}}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom toqito.measurements import pretty_bad_measurement, pretty_good_measurement\nfrom toqito.state_opt import state_distinguishability, state_exclusion\nfrom toqito.states import trine\n\n\ndef calculate_success_prob(\n    states: list[np.ndarray],\n    probs: list[float],\n    povm_operators: list[np.ndarray],\n) -> float:\n    \"\"\"Calculate the success probability \u03a3 p\u1d62 Tr(\u03c1\u1d62 M\u1d62).\n\n    This helper is robust to `states` being either state vectors or density matrices.\n    \"\"\"\n    success_prob = 0\n    num_states = len(states)\n    for i in range(num_states):\n        state = states[i]\n        op = povm_operators[i]\n        # Check if input is a vector (pure state) or matrix (density matrix)\n        if state.ndim == 1 or (state.ndim == 2 and min(state.shape) == 1):\n            # It's a vector (or column/row vector)\n            state_vec = state.flatten()\n            prob_i = state_vec.conj().T @ op @ state_vec\n        else:\n            # It's a density matrix\n            prob_i = np.trace(op @ state)\n        success_prob += probs[i] * prob_i\n    return np.real(success_prob)\n\n\n# 1. Define the states and probabilities.\nstate_vectors = trine()\nk = len(state_vectors)\nprobs = [1 / k] * k\n\nprint(f\"Analyzing k={k} trine states with uniform probability.\")\n\n# 2. Compute the optimal benchmark values.\np_best, _ = state_distinguishability(state_vectors, probs)\np_worst, _ = state_exclusion(state_vectors, probs)\n\nprint(\"\\nOptimal Benchmarks:\")\nprint(f\"  P_Best  = {p_best:.4f} (Max discrimination probability)\")\nprint(f\"  P_Worst = {p_worst:.4f} (Min discrimination probability)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results for the optimal benchmarks show that the maximum possible success\nprobability is $2/3$, and the minimum is $0$. The PGM is known to be\noptimal for the trine states, so we expect $P_{\\text{PGM}} = P_{\\text{Best}}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 3. Compute the PGM and PBM operators.\npgm_operators = pretty_good_measurement(state_vectors, probs)\npbm_operators = pretty_bad_measurement(state_vectors, probs)\n\n# 4. Calculate PGM and PBM success probabilities.\np_pgm = calculate_success_prob(state_vectors, probs, pgm_operators)\np_pbm = calculate_success_prob(state_vectors, probs, pbm_operators)\n\nprint(\"\\nHeuristic Measurements:\")\nprint(f\"  P_PGM = {p_pgm:.4f}\")\nprint(f\"  P_PBM = {p_pbm:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the PGM achieves the optimal value. Our calculated value for the\nPBM is $1/6 \\approx 0.1667$, which is a good approximation of the true\nworst case of $0$.\n\nFinally, we can verify the core relationship between these two measurements\nand the full performance hierarchy stated previously.\n\n$$\nP_{\\text{Best}} \\ge P_{\\text{PGM}} \\ge \\frac{1}{k} \\ge P_{\\text{PBM}} \\ge P_{\\text{Worst}}\n$$\n\n5. Verify the core relationship and the hierarchy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "relation_lhs = p_pgm + (k - 1) * p_pbm\nprint(\"\\nVerifying P_PGM + (k-1)*P_PBM = 1:\")\nprint(f\"  {p_pgm:.4f} + ({k - 1})*{p_pbm:.4f} = {relation_lhs:.4f} -> {np.isclose(relation_lhs, 1)}\")\n\nprint(\"\\nVerifying hierarchy (P_Best >= P_PGM >= 1/k >= P_PBM >= P_Worst):\")\nprint(f\"  P_Best >= P_PGM:    {p_best:.4f} >= {p_pgm:.4f}  ->  {p_best >= p_pgm or np.isclose(p_best, p_pgm)}\")\nprint(f\"  P_PGM >= 1/k:       {p_pgm:.4f} >= {1 / k:.4f}  ->  {p_pgm >= 1 / k or np.isclose(p_pgm, 1 / k)}\")\nprint(f\"  1/k >= P_PBM:       {1 / k:.4f} >= {p_pbm:.4f}  ->  {1 / k >= p_pbm or np.isclose(1 / k, p_pbm)}\")\nprint(f\"  P_PBM >= P_Worst:   {p_pbm:.4f} >= {p_worst:.4f} ->  {p_pbm >= p_worst or np.isclose(p_pbm, p_worst)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The verifications confirm that all theoretical relationships hold true for the\ntrine states.\n\nNow we can move on to visualizing the performance for the more general case\nof random states.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Performance on Random States\n\nFigures 4 and 5 from McIrvin et. al [@mcirvin2024pretty] show that for many randomly generated\nstates, the PGM and PBM probabilities cluster around the blind guessing\nbaseline of $1/k$. We can reproduce a similar plot.\n\nWe will generate 100 random ensembles of $k=4$ qubit states and plot\nthe resulting $P_{\\text{PGM}}$ and $P_{\\text{PBM}}$ values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfrom toqito.rand import random_density_matrix\n\n# Number of random ensembles to generate.\nnum_instances = 100\nk = 4  # Number of states in each ensemble.\ndim = 2  # Dimension of states (qubits).\n\npgm_results = []\npbm_results = []\n\nfor i in range(num_instances):\n    # Generate a random ensemble of k density matrices.\n    rand_states = [random_density_matrix(dim, seed=(i * k) + j) for j in range(k)]\n    # Generate random prior probabilities.\n    rand_probs = np.random.dirichlet(np.ones(k))\n\n    # Calculate PGM and PBM probabilities.\n    pgm_ops = pretty_good_measurement(rand_states, rand_probs)\n    pbm_ops = pretty_bad_measurement(rand_states, rand_probs)\n\n    pgm_prob = calculate_success_prob(rand_states, rand_probs, pgm_ops)\n    pbm_prob = calculate_success_prob(rand_states, rand_probs, pbm_ops)\n    pgm_results.append(pgm_prob)\n    pbm_results.append(pbm_prob)\n\n# Create the plot.\nfig, ax = plt.subplots(figsize=(8, 5), dpi=100)\nsample_indices = range(num_instances)\nax.scatter(sample_indices, pgm_results, alpha=0.7, label=\"$P_{PGM}$\", c=\"blue\", s=20)\nax.scatter(sample_indices, pbm_results, alpha=0.7, label=\"$P_{PBM}$\", c=\"red\", s=20)\n\n# Add blind guessing line for reference.\nblind_guess_prob = 1 / k\nax.axhline(\n    y=blind_guess_prob,\n    color=\"black\",\n    linestyle=\"--\",\n    label=f\"Blind Guessing (1/k = {blind_guess_prob:.2f})\",\n)\n\nax.set_xlabel(\"Random Instance Index\", fontsize=12)\nax.set_ylabel(\"Discrimination Success Probability\", fontsize=12)\nax.set_title(f\"PGM and PBM Performance for {num_instances} Random Ensembles (k={k})\", fontsize=14)\nax.legend()\nax.grid(True, linestyle=\":\", alpha=0.6)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot clearly illustrates the theoretical bounds. Every blue dot\nrepresenting $P_{\\text{PGM}}$ lies on or above the blind guessing line, and every\nred dot representing $P_{\\text{PBM}}$ lies on or below it. This provides strong\nnumerical evidence for the inequalities\n$P_{\\text{PGM}} \\ge 1/k \\ge P_{\\text{PBM}}$, confirming that the PGM is\nalways a better-than-random guess and the PBM is always a worse-than-random\nguess for state discrimination.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}